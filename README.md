# Abstractive Text Summarizer with GAN-like Training

This project implements an abstractive text summarizer using a pre-trained BART model as the generator and a custom-trained discriminator in a Generative Adversarial Network (GAN)-like setup. The goal is to improve the quality of generated summaries by incorporating adversarial feedback.

## Project Overview

The core components are:

*   **Generator**: A `facebook/bart-large-cnn` model, fine-tuned for abstractive summarization.
*   **Discriminator**: A custom neural network (defined in `discriminator.py`) trained to distinguish between human-written summaries and summaries generated by the BART model.
*   **Dataset**: The CNN/Daily Mail dataset is used for training and evaluation. A small subset (20 samples) is used in the current configuration for demonstration purposes.

## Training Process

The training involves alternately training the discriminator and the generator:

1.  **Discriminator Training**:
    *   The discriminator is trained to classify summaries as either "real" (human-written from the dataset) or "fake" (generated by the BART model).
    *   It uses a standard cross-entropy loss for this binary classification task.

2.  **Generator Training**:
    *   The generator (BART model) is trained with a combined loss function:
        *   **Policy Gradient Loss (J_pg)**: This loss component encourages the generator to produce summaries that the discriminator classifies as "real". The reward signal comes from the discriminator's probability that a generated summary is real.
        *   **Maximum Likelihood Estimation (MLE) Loss**: This is the standard cross-entropy loss, where the generator is trained to predict the human-written reference summaries.
    *   A balance factor (`beta`) controls the contribution of each loss component.

## Evaluation

The performance of the generator is evaluated using the **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** metric.
*   Generated summaries for a test set are compared against human-written reference summaries.
*   ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) are calculated to measure overlap and quality.
*   Additionally, the summary generated for the first article in the test set is saved to `first_article_summaries.txt` at each epoch for qualitative comparison.

## How to Run

1.  Ensure you have the necessary Python packages installed (e.g., `torch`, `transformers`, `datasets`, `evaluate`).
2.  To run the training and evaluation pipeline, execute the main script:
    ```bash
    python main.py
    ```

Model checkpoints (`generator_checkpoint.pth` and `discriminator_checkpoint.pth`) will be saved after each epoch and loaded if they exist when the script is re-run.
